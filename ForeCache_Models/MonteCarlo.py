"""
General purpose Monte Carlo model for training on-policy methods.
"""
from copy import deepcopy
import numpy as np


class FiniteMCModel:
    def __init__(self, state_space, action_space, gamma=1.0, epsilon=0.1):
        """MCModel takes in state_space and action_space (finite)
        Arguments
        ---------

        state_space: int OR list[observation], where observation is any hashable type from env's obs.
        action_space: int OR list[action], where action is any hashable type from env's actions.
        gamma: float, discounting factor.
        epsilon: float, epsilon-greedy parameter.

        If the parameter is an int, then we generate a list, and otherwise we generate a dictionary.
        >>> m = FiniteMCModel(2,3,epsilon=0)
        >>> m.Q
        [[0, 0, 0], [0, 0, 0]]
        >>> m.Q[0][1] = 1
        >>> m.Q
        [[0, 1, 0], [0, 0, 0]]
        >>> m.pi(1, 0)
        1
        >>> m.pi(1, 1)
        0
        >>> d = m.generate_returns([(0,0,0), (0,1,1), (1,0,1)])
        >>> assert(d == {(1, 0): 1, (0, 1): 2, (0, 0): 2})
        >>> m.choose_action(m.pi, 1)
        0
        """
        self.gamma = gamma
        self.epsilon = epsilon
        self.Q = None
        if isinstance(action_space, int):
            self.action_space = np.arange(action_space)
            actions = [0] * action_space
            # Action representation
            self._act_rep = "list"
        else:
            self.action_space = action_space
            actions = {k: 0 for k in action_space}
            self._act_rep = "dict"
        if isinstance(state_space, int):
            self.state_space = np.arange(state_space)
            self.Q = [deepcopy(actions) for _ in range(state_space)]
        else:
            self.state_space = state_space
            self.Q = {k: deepcopy(actions) for k in state_space}

        # Frequency of state/action.
        self.Ql = deepcopy(self.Q)

    def pi(self, action, state):
        """pi(a,s,A,V) := pi(a|s)
        We take the argmax_a of Q(s,a).
        q[s] = [q(s,0), q(s,1), ...]
        """
        if self._act_rep == "list":
            if action == np.argmax(self.Q[state]):
                return 1
            return 0
        elif self._act_rep == "dict":
            if action == max(self.Q[state], key=self.Q[state].get):
                return 1
            return 0

    def b(self, action, state):
        """b(a,s,A) := b(a|s)
        Sometimes you can only use a subset of the action space
        given the state.

        Randomly selects an action from a uniform distribution.
        """
        return self.epsilon / len(self.action_space) + (1 - self.epsilon) * self.pi(action, state)

    def generate_returns(self, ep):
        """Backup on returns per time period in an epoch
        Arguments
        ---------

        ep: [(observation, action, reward)], an episode trajectory in chronological order.
        """
        G = {}  # return on state
        C = 0  # cumulative reward
        for tpl in reversed(ep):
            observation, action, reward = tpl
            G[(observation, action)] = C = reward + self.gamma * C
        return G

    def choose_action(self, policy, state):
        """Uses specified policy to select an action randomly given the state.
        Arguments
        ---------

        policy: function, can be self.pi, or self.b, or another custom policy.
        state: observation of the environment.
        """
        probs = [policy(a, state) for a in self.action_space]   #randomly choosing action porobs =[0.5,0.5]
        return np.random.choice(self.action_space, p=probs)

    def update_Q(self, ep):
        """Performs a action-value update.
        Arguments
        ---------

        ep: [(observation, action, reward)], an episode trajectory in chronological order.
        """
        # Generate returns, return ratio
        G = self.generate_returns(ep)
        for s in G:
            state, action = s
            q = self.Q[state][action]
            self.Ql[state][action] += 1
            N = self.Ql[state][action]
            self.Q[state][action] = q * N / (N + 1) + G[s] / (N + 1)

    def score(self, env, policy, n_samples=1000):
        """Evaluates a specific policy with regards to the env.
        Arguments
        ---------

        env: an openai gym env, or anything that follows the api.
        policy: a function, could be self.pi, self.b, etc.
        """
        rewards = []
        overall_accuracy =[]
        for sam in range(n_samples):
            print( "##############################",sam )
            observation = env.reset(test=True)  #test environment start from training data
            cum_rewards = 0
            sample_predictions=[]
            while True:
                action = self.choose_action(policy, observation)
                print(action)
                #prediction = check in test
                observation, reward, done, info = env.step(action)
                sample_predictions.append(info['isPredictionCorrect'])
                cum_rewards += reward
                if done:
                    rewards.append(cum_rewards)
                    overall_accuracy.append(np.mean(sample_predictions))#get sample accuracy and add it to overall accuracy per sample
                    break
        return np.mean(rewards), overall_accuracy


if __name__ == "__main__":
    import doctest

    doctest.testmod()